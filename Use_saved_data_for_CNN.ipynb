{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Use_saved_data_for_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1TTUIgdoVBQY5NrvfnpmgHuQ3EGlBbFQt",
      "authorship_tag": "ABX9TyN553labHUGb4cy0bY2YXOk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I have augmented the images that I will be classifying.  I created a function to resize, grayscale, and normalize the pixel values of each image.  Then I created a dataframe combining the images with their labels.  This uncovered the fact that about 9,000 of the 50,000 images were mislabeled.  The jpg name of the image could not find a partner jpg name in the labels csv provided. (This raises the question of whether a clustering algorithm could be useful for the unlabeled data.)\n",
        "\n",
        "Using the reduced data set, I split the data into stratified test and validation sets and saved them to disk as numpy arrays. The data started at over 11 Gbs in size which caused RAM issues on my local machine of only 16 Gb RAM. I could load the data but would run out of memory when trying to manipulate it. I solved this by using Google Colab Pro, which provides me with 25 GB of RAM.  This is still not enough to manipulate the data as much as I would like, but I have been working around it by making one change and saving it, then uploading that data to a new notebook with a fresh 25 GB to start.  My final training data is about 9 GB in size.  I may need to augment the images further through rotations and/or larger sizes to get more detail, but that will require even more RAM so I will start with the 9GB set.\n",
        "\n",
        "I have created a base model with 94% training accuracy but only 67% validation accuracy.  The model is overfitting, but it gives me a starting point.\n",
        "\n",
        "Changing leaky Relu to Relu improved validation accuracy to 70%\n",
        "\n",
        "Dense layer neurons from 3,000 to 300 reduced validation accuracy to 69%\n",
        "\n",
        "Reducing layers from 3 to 2 convolutional and pooling reduced validation accuracy to 65%\n",
        "\n",
        "Adding one additional convolutional layer of 256 and pooling layer increased validation accuracy to 74%\n",
        "\n",
        "Adding another additional convolutional layer of 512 and a pooling layer increased validation accuracy to 76%\n",
        "\n",
        "Another convolutional layer of 1024 did not improve accuracy\n"
      ],
      "metadata": {
        "id": "QsYh9oc7k7LC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lElzJZ3RKemg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import asarray\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=np.load('drive/MyDrive/X_train_strat.npy')"
      ],
      "metadata": {
        "id": "DEAUNeELMeaG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=np.load('drive/MyDrive/y_train_strat.npy')"
      ],
      "metadata": {
        "id": "GNPlaP3rM5wK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val=np.load('drive/MyDrive/X_val_strat.npy')"
      ],
      "metadata": {
        "id": "9X7YELBbidrY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val=np.load('drive/MyDrive/y_val_strat.npy')"
      ],
      "metadata": {
        "id": "6eH-O8hdid44"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei88D6ySNJ83",
        "outputId": "872decd4-c6ca-4feb-cf56-74fa5837aef4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((33089, 150, 200), (33089, 28))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline Model\n",
        "\n",
        "# from tensorflow.keras import layers\n",
        "# from keras.models import Sequential,Input,Model\n",
        "# from keras.layers import Dense, Dropout, Flatten\n",
        "# from keras.layers import Conv2D, MaxPooling2D\n",
        "# from keras.layers.advanced_activations import LeakyReLU\n",
        "\n",
        "# model = keras.models.Sequential()\n",
        "# model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(150,200,1),padding='same'))\n",
        "# model.add(LeakyReLU(alpha=0.1))\n",
        "# model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "# model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
        "# model.add(LeakyReLU(alpha=0.1))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "# model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
        "# model.add(LeakyReLU(alpha=0.1))                  \n",
        "# model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(3000, activation='linear'))\n",
        "# model.add(LeakyReLU(alpha=0.1))                  \n",
        "# model.add(Dense(28, activation='softmax'))"
      ],
      "metadata": {
        "id": "EyHCmOTHNM7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wa_pdmR7OBWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model.fit(X_train, y_train,batch_size=30, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cchgS0GzOQat",
        "outputId": "2a4ba25d-89a4-4ed3-e7d1-f399c5abfd51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1103/1103 [==============================] - 56s 39ms/step - loss: 1.4638 - accuracy: 0.5738\n",
            "Epoch 2/5\n",
            "1103/1103 [==============================] - 43s 39ms/step - loss: 0.9173 - accuracy: 0.7120\n",
            "Epoch 3/5\n",
            "1103/1103 [==============================] - 44s 39ms/step - loss: 0.5395 - accuracy: 0.8241\n",
            "Epoch 4/5\n",
            "1103/1103 [==============================] - 43s 39ms/step - loss: 0.2643 - accuracy: 0.9122\n",
            "Epoch 5/5\n",
            "1103/1103 [==============================] - 43s 39ms/step - loss: 0.1778 - accuracy: 0.9439\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee201680d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##model.evaluate(X_val,y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJXe5380PyzS",
        "outputId": "ee947952-f675-4575-d71e-3f8a9e473774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "259/259 [==============================] - 3s 11ms/step - loss: 1.8553 - accuracy: 0.6786\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.855276346206665, 0.6785930395126343]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Baseline model not stratified.  Training accuracy 0.95   evaluation accuracy 0.65\n",
        "##### Stratified Baseline model       Training accuracy 0.94   evaluation accuracy 0.67\n",
        "##### Stratified Relu, not leaky relu Training accuracy 0.96   evaluation accuracy 0.70"
      ],
      "metadata": {
        "id": "Aqm437sOhzpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model2  not leaky Relu"
      ],
      "metadata": {
        "id": "REtxiNCii20p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "model2 = keras.models.Sequential()\n",
        "model2.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(150,200,1),padding='same'))\n",
        "model2.add(MaxPooling2D((2, 2),padding='same'))\n",
        "model2.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "model2.add(Conv2D(128, (3, 3), activation='relu',padding='same'))                 \n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "model2.add(Conv2D(256, (3, 3), activation='relu',padding='same'))                 \n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "model2.add(Conv2D(512, (3, 3), activation='relu',padding='same'))                 \n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(300, activation='relu'))               \n",
        "model2.add(Dense(28, activation='softmax'))"
      ],
      "metadata": {
        "id": "qVzLD-txWfLK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uyA2sDY3cdyI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(X_train, y_train,batch_size=30, epochs=5, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CDyz1qrcd0W",
        "outputId": "adb1957f-e886-4cb5-9929-8559bf1c9764"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1103/1103 [==============================] - 52s 32ms/step - loss: 1.4476 - accuracy: 0.5654 - val_loss: 1.0599 - val_accuracy: 0.6831\n",
            "Epoch 2/5\n",
            "1103/1103 [==============================] - 28s 25ms/step - loss: 0.9348 - accuracy: 0.7101 - val_loss: 0.9696 - val_accuracy: 0.7018\n",
            "Epoch 3/5\n",
            "1103/1103 [==============================] - 28s 25ms/step - loss: 0.7218 - accuracy: 0.7721 - val_loss: 0.7984 - val_accuracy: 0.7550\n",
            "Epoch 4/5\n",
            "1103/1103 [==============================] - 28s 25ms/step - loss: 0.5527 - accuracy: 0.8229 - val_loss: 0.7893 - val_accuracy: 0.7639\n",
            "Epoch 5/5\n",
            "1103/1103 [==============================] - 27s 25ms/step - loss: 0.4013 - accuracy: 0.8694 - val_loss: 0.8490 - val_accuracy: 0.7673\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f28b052c850>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.evaluate(X_val,y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiQQA5Q7cd2O",
        "outputId": "f9e956bf-b945-4f60-a38f-799a186812bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "259/259 [==============================] - 3s 11ms/step - loss: 1.6980 - accuracy: 0.7037\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6980047225952148, 0.7037350535392761]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P2tUJ0XPcd50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model3"
      ],
      "metadata": {
        "id": "emQ6cp9Ci90b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "model3 = keras.models.Sequential()\n",
        "model3.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(150,200,1),padding='same'))\n",
        "model3.add(MaxPooling2D((2, 2),padding='same'))\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu',padding='same'))                 \n",
        "model3.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "model3.add(Conv2D(256, (3, 3), activation='relu',padding='same'))                 \n",
        "model3.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "model3.add(Conv2D(512, (3, 3), activation='relu',padding='same'))                 \n",
        "model3.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(300, activation='relu'))               \n",
        "model3.add(Dense(28, activation='softmax'))"
      ],
      "metadata": {
        "id": "t8NIOYCfjADb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "u5_HgisOpAvW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.fit(X_train, y_train,batch_size=30, epochs=8, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H5bfQslpA0K",
        "outputId": "befe2e6b-500f-4127-8766-ee0afab8bec6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "1103/1103 [==============================] - 69s 47ms/step - loss: 1.5289 - accuracy: 0.5384 - val_loss: 1.1463 - val_accuracy: 0.6489\n",
            "Epoch 2/8\n",
            "1103/1103 [==============================] - 47s 43ms/step - loss: 0.9480 - accuracy: 0.7079 - val_loss: 0.8872 - val_accuracy: 0.7273\n",
            "Epoch 3/8\n",
            "1103/1103 [==============================] - 47s 43ms/step - loss: 0.7432 - accuracy: 0.7643 - val_loss: 0.8410 - val_accuracy: 0.7392\n",
            "Epoch 4/8\n",
            "1103/1103 [==============================] - 47s 43ms/step - loss: 0.5752 - accuracy: 0.8153 - val_loss: 0.8231 - val_accuracy: 0.7520\n",
            "Epoch 5/8\n",
            "1103/1103 [==============================] - 47s 42ms/step - loss: 0.4218 - accuracy: 0.8619 - val_loss: 0.8628 - val_accuracy: 0.7576\n",
            "Epoch 6/8\n",
            "1103/1103 [==============================] - 47s 42ms/step - loss: 0.2974 - accuracy: 0.9007 - val_loss: 0.9131 - val_accuracy: 0.7660\n",
            "Epoch 7/8\n",
            "1103/1103 [==============================] - 47s 42ms/step - loss: 0.2088 - accuracy: 0.9304 - val_loss: 1.0835 - val_accuracy: 0.7608\n",
            "Epoch 8/8\n",
            "1103/1103 [==============================] - 47s 42ms/step - loss: 0.1603 - accuracy: 0.9461 - val_loss: 1.2281 - val_accuracy: 0.7452\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6320417dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fd4Y_lG6uP_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}